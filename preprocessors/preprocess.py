# -*- coding=utf-8 -*-
# @File     : preprocess.py
# @Time     : 2023/8/2 18:32
# @Author   : EvanHong
# @Email    : 939778128@qq.com
# @Project  : 2023.06.08è¶…é«˜é¢‘ä¸Šè¯50æŒ‡æ•°è®¡ç®—
# @Description: éœ€è¦ä¿è¯è¯¥æ–‡ä»¶cleanï¼Œä¸è¦åŠ å…¥è¿‡å¤šæ— ç”¨ç®—æ³•
import logging
import re
from copy import deepcopy
from datetime import timedelta

import numpy as np
import pandas as pd
from numba import jit
from sklearn.preprocessing import StandardScaler

from config import *
import config
from support import LobColTemplate, Target
import pickle
from abc import abstractmethod


class BasePreprocessor(object):
    def __init__(self, *args, **kwargs):
        self.args = args
        for key, value in kwargs.items():
            self.__setattr__(key, value)


class BaseDataPreprocessor(BasePreprocessor):
    """
    ä¸»è¦ç”¨äºå¯¹ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥é¢„å¤„ç†ï¼Œä»è€Œé€‚åº”å„ä¸ªæ¨¡å‹
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scaler = StandardScaler()

    def std_scale(self, data, refit=True, *args):
        res = []

        def _meta(data: pd.DataFrame, scaler):
            cols = data.columns
            index = data.index
            return pd.DataFrame(scaler.transform(data), index=index, columns=cols)

        if refit: self.scaler.fit(data)
        data = _meta(data, self.scaler)
        res.append(data)
        for other in args:
            other = _meta(other, self.scaler)
            res.append(other)
        return tuple(res)

    def sub_illegal_punctuation(self, string):
        return re.sub('\W+', '_', string)

    def save_scaler(self, dir, file_name):
        # file_name=re.sub('\W+','_', file_name)
        with open(dir + file_name, 'wb') as f:
            pickle.dump(self.scaler, f)

    def load_scaler(self, dir, file_name):
        # file_name=re.sub('\W+','_', file_name)
        with open(dir + file_name, 'rb') as f:
            self.scaler = pickle.load(f)

    @classmethod
    def get_flattened_Xy(cls, *args, **kwargs):
        pass

    @abstractmethod
    def get_stacked_Xy(self, *args, **kwargs):
        """
        ä¾‹å¦‚5è¡ŒXå¯¹åº”1è¡Œy
        Parameters
        ----------
        args
        kwargs

        Returns
        -------

        """
        pass

    @abstractmethod
    def align_Xy(self, *args, **kwargs):
        pass


class ShiftDataPreprocessor(BaseDataPreprocessor):
    """
    å¯¹featureè¿›è¡Œå¹³ç§»æ‹¼æ¥ï¼Œç”Ÿæˆå¯è¾“å…¥æ¨¡å‹çš„æ•°æ®

    NOTE
    ----
        å¾ˆå¯èƒ½å¯¼è‡´ç»´åº¦çˆ†ç‚¸ï¼Œå› ä¸ºå¢åŠ ä½¿ç”¨çš„æ•°æ®ç‚¹çš„æ•°é‡ï¼Œä¼šä½¿å¾—æœ€ç»ˆç‰¹å¾æ•°å‘ˆçº¿æ€§å€æ•°å¢é•¿

    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @staticmethod
    def shift_X(X, use_n_steps):
        # todo æŠŠè¯¥å¤„ç†å•ç‹¬åšå‡½æ•°
        XX = pd.concat(
            [X.shift(i).rename(columns={col: col + f'_lag{i}' for col in X.columns}) for i in
             range(use_n_steps)],
            axis=1)
        return XX

    @classmethod
    def get_flattened_Xy(cls, alldatas, num, target, pred_n_steps, use_n_steps, drop_current):
        if target == Target.ret.name:
            current = alldatas[num]['current']
            y = np.log(current.shift(-pred_n_steps) / current)
            y = y.rename(f'{target}_{pred_n_steps * 0.2}s')
        elif target == Target.mid_p_ret.name:
            mid_price = alldatas[num]['mid_price']
            y = np.log(mid_price.shift(-pred_n_steps) / mid_price)
            y = y.rename(f'{target}_{pred_n_steps * 0.2}s')
        else:
            raise NotImplementedError("align_Xy target not defined")

        if drop_current:
            X = alldatas[num].drop(columns=['current'])
        else:
            X = alldatas[num]

        X = cls.shift_X(X, use_n_steps)

        # x yå·²ç»å¯¹é½ï¼Œåªéœ€è¦è¿›è¡ŒåŒæ ·çš„è£å‰ªï¼Œå»æ‰nanå°±å¯ä»¥
        y = y.iloc[use_n_steps:-pred_n_steps]
        X = X.iloc[use_n_steps:-pred_n_steps]

        return X, y


class AggDataPreprocessor(BaseDataPreprocessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @staticmethod
    def realized_volatility(series: pd.Series):
        return np.sqrt(np.sum(series ** 2))

    @staticmethod
    def diff(series: pd.Series):
        return series.iloc[-1] - series.iloc[0]

    def agg_features(self, features: pd.DataFrame):
        """
        ç”¨äºå°†éå¸¸é«˜é¢‘çš„æ•°æ®aggä¸ºé«˜åº¦æŠ½è±¡çš„ç‰¹å¾ï¼Œå¦‚meanã€stdã€realized volç­‰ï¼Œå¹¶ä¸”å¯¹aggçš„å¤§å°ä¸åŒå¯ä»¥æ„é€ å‡ºåŠ¨é‡ç‰¹å¾
        Parameters
        ----------
        features
            ['ALO','AMO','ATT','BLO','BMO','BTT','a1_p','a1_v','a21_p_gap','a2_p','a2_v','a32_p_gap','a3_p','a3_v','a43_p_gap','a4_p','a4_v','a54_p_gap','a5_p','a5_v','b1_p','b1_v','b21_p_gap','b2_p','b2_v','b32_p_gap','b3_p','b3_v','b43_p_gap','b4_p','b4_v','b54_p_gap','b5_p','b5_v','bid_ask_volume_ratio1','bid_ask_volume_ratio2','bid_ask_volume_ratio3','bid_ask_volume_ratio4','bid_ask_volume_ratio5','buy_sell_pressure','cum_turnover','cum_vol','current','hi_2','hi_3','hi_4','li_1','li_2','li_3','li_4','mid_price','price_breadth_a','price_breadth_b','relative_spread','spread','spread_tick','turnover','voi','volume','wap1','wap1_c','wap2','wap2_c','wap3','wap3_c','wap4','wap4_c','wap5','wap5_c']
        Returns
        -------

        """

        # features = features.resample(agg_freq).agg([np.mean, np.std, np.median])
        # @jit
        def _calc_decay_weight(m, H):
            """è®¡ç®—è¡°å‡åŠ æƒæƒé‡
            è€ƒè™‘åˆ°è·ç¦»å½“å‰æ—¶é—´è¾ƒè¿œæ—¥çš„æ•°æ®å¯¹äºå½“å‰å› å­å–å€¼çš„å½±å“åº”è¯¥æ›´å°ï¼Œè€Œè¿‘æœŸæ•°æ®çš„å½±å“åº”è¯¥æ›´å¤§ï¼Œå› æ­¤æˆ‘ä»¬åœ¨åŠ æƒæ”¶ç›Šä¸Šè¿›è¡Œæ—¶é—´åŠè¡°çš„æƒé‡å€¾æ–œ

            Parameters
            ----------
            m:
                æ”¶ç›ŠåŠ æƒçš„çª—å£é•¿åº¦
            H:
                ğ» ä¸ºåŠè¡°æœŸ

            Returns
            -------

            References
            ----------
            [1] 2023-07-14_ä¸œæ–¹è¯åˆ¸_å› å­é€‰è‚¡ç³»åˆ—ä¹‹ä¹åå››ï¼šUMR2.0ï¼Œé£é™©æº¢ä»·è§†è§’ä¸‹çš„åŠ¨é‡åè½¬ç»Ÿä¸€æ¡†æ¶å†å‡çº§.pdf
            """
            weights = np.array([np.power(2, -(m - j + 1) / H) for j in range(1, m + 1)])
            weights = weights / sum(weights)
            return weights

        # @jit
        def momentum_UMR(features: pd.DataFrame):
            """

            Parameters
            ----------
            features:
                æ‰€æœ‰ç‰¹å¾

            split:
                åˆ†ä¸ºnç­‰åˆ†ï¼Œè®¡ç®—é£é™©è°ƒæ•´åŠ æƒæ”¶ç›Š

            Returns
            -------

            References
            ----------
            [1] 2023-07-14_ä¸œæ–¹è¯åˆ¸_å› å­é€‰è‚¡ç³»åˆ—ä¹‹ä¹åå››ï¼šUMR2.0ï¼Œé£é™©æº¢ä»·è§†è§’ä¸‹çš„åŠ¨é‡åè½¬ç»Ÿä¸€æ¡†æ¶å†å‡çº§.pdf
            """
            if len(features) == 0: return pd.DataFrame({
                'risk_avg_vol': [np.nan],
                'risk_ret_std': [np.nan],
                'risk_ret_skew': [np.nan],
            })
            split = 5
            H = 2
            agg_rows = step_rows = int(len(features) / split)
            weights = _calc_decay_weight(split, H=2)
            # weights = np.array([np.power(2, -(m - j + 1) / H) for j in range(1, m + 1)])
            # weights = weights / sum(weights)
            _features = features.iloc[::step_rows]
            returns = _features['wap1_ret']
            # è®¡ç®—é£é™©ç³»æ•°riskï¼Œä¸åŒä»£ç†å˜é‡æœ‰ä¸åŒè®¡ç®—æ–¹æ³•
            risk_avg_vol = features['volume'].rolling(agg_rows, min_periods=agg_rows, step=step_rows).mean() - \
                           _features['volume']
            risk_ret_std = (features['wap1_ret']*100000).rolling(agg_rows, min_periods=agg_rows, step=step_rows).std() - \
                           (_features['wap1_ret']*100000)
            risk_ret_skew = features['wap1_ret'].rolling(agg_rows, min_periods=agg_rows, step=step_rows).skew() - \
                            _features['wap1_ret']

            UMR = pd.DataFrame({
                'risk_avg_vol': [np.sum(weights * risk_avg_vol * returns)],
                'risk_ret_std': [np.sum(weights * risk_ret_std * returns)],
                'risk_ret_skew': [np.sum(weights * risk_ret_skew * returns)],
            })
            return UMR

        agg_rows = int(agg_timedelta / min_timedelta)
        step_rows = int(pred_timedelta / min_timedelta)
        agg_mapper = {k: [np.mean, np.std] for k in features.columns}
        agg_diff = {k: [self.diff] for k in
                    ['cum_turnover', 'cum_vol']}  # ç´¯ç§¯æˆäº¤é‡ä¸éœ€è¦æ±‚å¹³å‡ï¼Œä¹Ÿæ— æ³•æŒ‰ç…§last nè¿›è¡Œå¹³å‡ç”¨äºåæ˜ åŠ¨é‡ï¼Œå¯è¡Œçš„æ–¹æ³•ä¸º æœ«å€¼-åˆå€¼
        agg_rv = {k: [self.realized_volatility] for k in ['wap1_ret', 'wap2_ret', 'mid_p_ret']}
        agg_mapper.update(agg_diff)
        agg_mapper.update(agg_rv)

        # åŠ å…¥realized volå› å­ï¼Œéœ€è¦æ³¨æ„è¯¥dfæ˜¯2 level header
        last_n_rows = [agg_rows, int(agg_rows / 2)] # todo æ˜¯å¦éœ€è¦è¿™æ ·æ¥æ„é€ åŠ¨é‡
        # last_n_rows = [agg_rows]
        dfs = []
        for last_n in last_n_rows:
            agg_features = features.rolling(last_n, min_periods=last_n, step=step_rows, closed='left',
                                            center=False).agg(agg_mapper)  # åˆ é™¤äº†median
            agg_features.columns = ['_'.join(col) for col in agg_features.columns]

            mom_features = pd.concat([momentum_UMR(table) for table in
                            features.rolling(last_n, min_periods=last_n, step=step_rows, closed='left', center=False,
                                             method='table')])

            mom_features.index=agg_features.index
            temp = pd.concat([agg_features, mom_features], axis=1)
            temp.columns = [col + '_' + str(last_n) for col in temp.columns]
            dfs.append(temp)

        agg_features = pd.concat(dfs, axis=1)

        return agg_features

    @classmethod
    def get_flattened_Xy(cls, alldatas, num, target, pred_n_steps, use_n_steps, drop_current):
        pass

    def align_Xy(self, X, y, pred_timedelta):
        start_time = X.index
        tar_time = start_time + pred_timedelta

        available_time = [True if t in y.index else False for t in tar_time]
        start_time = start_time[available_time]
        tar_time = tar_time[available_time]

        X = X.loc[start_time]
        y = y.loc[tar_time]

        return X, y


class LobCleanObhPreprocessor(BasePreprocessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @staticmethod
    def split_price(order_book_history: pd.DataFrame, window_size=-1):
        """
        å°†order_book_historyåˆ†è§£ä¸ºpriceå’Œvolumeçš„ä¸¤ä¸ª2d dataframes
        :param window_size: å‡ æ¡£ç›˜å£ä¿¡æ¯ï¼Œ10ä»£è¡¨ä¹°å–å„10æ¡£
        :param order_book_history:
        :return:
        """

        def meta(row: pd.Series):
            return row.dropna().index.tolist()

        temp = pd.DataFrame(order_book_history.apply(lambda x: meta(x), axis=1, result_type="expand"))
        origin_level = int(temp.shape[1] / 2)
        temp.columns = [f'a{i}_p' for i in range(origin_level, 0, -1)] + [f'b{i}_p' for i in range(1, origin_level + 1)]
        if window_size == -1:
            window_size = origin_level

        levels = [f'a{i}_p' for i in range(window_size, 0, -1)] + [f'b{i}_p' for i in range(1, window_size + 1)]

        temp = temp.loc[:, levels]
        temp.index = order_book_history.index
        return temp

    @staticmethod
    def split_volume(order_book_history: pd.DataFrame, window_size=-1):
        """
        å°†order_book_historyåˆ†è§£ä¸ºpriceå’Œvolumeçš„ä¸¤ä¸ª2d dataframes
        :param window_size: å‡ æ¡£ç›˜å£ä¿¡æ¯ï¼Œ10ä»£è¡¨ä¹°å–å„10æ¡£
        :param order_book_history:
        :return:
        """

        def meta(row: pd.Series):
            return row.dropna().values

        temp = pd.DataFrame(order_book_history.apply(lambda x: meta(x), axis=1, result_type="expand"))
        origin_level = int(temp.shape[1] / 2)
        temp.columns = [f'a{i}_v' for i in range(origin_level, 0, -1)] + [f'b{i}_v' for i in range(1, origin_level + 1)]
        if window_size == -1:
            window_size = origin_level
        levels = [f'a{i}_v' for i in range(window_size, 0, -1)] + [f'b{i}_v' for i in range(1, window_size + 1)]
        temp = temp.loc[:, levels]
        temp.columns = levels
        temp.index = order_book_history.index
        return temp

    @staticmethod
    def _gen_clean_obh(datafeed, snapshot_window):
        """

        :return:
        """
        order_book_history = datafeed.order_book_history
        current = datafeed.current
        assert len(order_book_history) > 0
        obh = order_book_history
        try:
            obh.index = pd.to_datetime(obh.index)
        except:
            obh = obh.T
            obh.index = pd.to_datetime(obh.index)
        obh = obh.sort_index(ascending=True)
        obh.columns = obh.columns.astype(float)
        obh_v = LobCleanObhPreprocessor.split_volume(obh, window_size=snapshot_window)
        obh_p = LobCleanObhPreprocessor.split_price(obh, window_size=snapshot_window)
        mid_p = (obh_p[str(LobColTemplate('a', 1, 'p'))] + obh_p[str(LobColTemplate('a', 1, 'p'))]).rename('mid_p') / 2

        clean_obh = pd.concat([obh_p, obh_v, current, mid_p], axis=1).ffill().bfill()
        clean_obh.index = pd.to_datetime(clean_obh.index)
        clean_obh = LobTimePreprocessor.del_untrade_time(clean_obh, cut_tail=True)
        clean_obh = LobTimePreprocessor.add_head_tail(clean_obh,
                                                      head_timestamp=config.important_times[
                                                          'continues_auction_am_start'],
                                                      tail_timestamp=config.important_times['continues_auction_pm_end'])

        return clean_obh

    @staticmethod
    def save_clean_obh(clean_obh, file_root, date, stk_name):
        clean_obh.to_csv(file_root + FILE_FMT_clean_obh.format(date, stk_name))

    @staticmethod
    def gen_and_save(datafeed, save_root, date: str, stk_name: str, snapshot_window):
        clean_obh = LobCleanObhPreprocessor._gen_clean_obh(datafeed, snapshot_window)
        LobCleanObhPreprocessor.save_clean_obh(clean_obh, save_root, date, stk_name)

    def run_batch(self):
        """
        æ‰¹é‡å¤„ç†
        :return:
        """
        pass


class LobTimePreprocessor(BasePreprocessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # self.freq = '200ms'

    @staticmethod
    def del_untrade_time(df, cut_tail=True):
        """

        :param df:
        :param cut_tail: å»æ‰å°¾ç›˜3minç«ä»·
        :return:
        """

        end_time = config.important_times['close_call_auction_end'] if not cut_tail else config.important_times[
            'continues_auction_pm_end']
        a = df.loc[
            config.important_times['continues_auction_am_start']:config.important_times['continues_auction_am_end']]
        b = df.loc[config.important_times['continues_auction_pm_start']:end_time]
        temp = pd.concat([a, b], axis=0)

        temp = temp.sort_index()
        return temp

    @staticmethod
    def add_head_tail(df, head_timestamp, tail_timestamp):
        # df=df.dropna(how='any',axis=0)
        try:
            # print(df.index.dtype, df)
            assert df.index[0] >= head_timestamp and df.index[-1] <= tail_timestamp
        except Exception as e:
            print('add_head_tail', df.index[0], head_timestamp, df.index[-1], tail_timestamp)
            raise e
        res = df.copy(deep=True)
        res.loc[pd.to_datetime(tail_timestamp)] = df.iloc[-1].copy(deep=True)
        res.loc[pd.to_datetime(head_timestamp)] = df.iloc[0].copy(deep=True)
        res = res.sort_index()
        return res

    @staticmethod
    def split_by_trade_period(df):
        res = [df.loc[s:e] for s, e in config.ranges]
        res = [LobTimePreprocessor.add_head_tail(cobh, head_timestamp=pd.to_datetime(s),
                                                 tail_timestamp=pd.to_datetime(e)) for cobh, (s, e) in
               zip(res, config.ranges)]
        return res

    @staticmethod
    def change_freq(df, freq):
        """deprecated"""
        logging.warning("change_freq deprecated", DeprecationWarning)
        return df.asfreq(freq='10ms', method='ffill').asfreq(freq=freq)

    @staticmethod
    def align_price(df, prices):
        pass

    # @staticmethod
    # def split_change_freq(df,freq='200ms'):
    #     dfs=LobTimePreprocessor.split_by_trade_period(df)
    #     clean_obhs


class LobFeatureEngineering(object):
    """

    """

    def __init__(self):
        self.ap = {k: str(LobColTemplate('a', k, 'p')) for k in range(1, 11)}
        self.bp = {k: str(LobColTemplate('b', k, 'p')) for k in range(1, 11)}
        self.av = {k: str(LobColTemplate('a', k, 'v')) for k in range(1, 11)}
        self.bv = {k: str(LobColTemplate('b', k, 'v')) for k in range(1, 11)}
        self.curr = 'current'

    def calc_buy_intense(self):
        """
        todo: ä¹°å…¥æ„æ„¿å› å­ã€Šé«˜é¢‘å› å­çš„ç°å®ä¸å¹»æƒ³ã€‹ï¼Œéœ€è¦åˆ©ç”¨9:30å¼€ç›˜ååŠå°æ—¶å†…çš„æ•°æ®æ„å»ºè¯¥å› å­
        :return:
        """
        pass

    def calc_wap(self, df, level, cross=True):
        """Function to calculate first WAP

        References
        ----------
        [1] optiveré‡‘ç‰Œç®—æ³•. https://mp.weixin.qq.com/s/Pe4i3I9-ErYFE9uL5B5pvQ
        """
        if cross:
            wap = (df[self.bp[level]] * df[self.av[level]] + df[self.ap[level]] * df[self.bv[level]]) / (
                    df[self.bv[level]] + df[self.av[level]])
        else:
            wap = (df[self.ap[level]] * df[self.av[level]] + df[self.bp[level]] * df[self.bv[level]]) / (
                    df[self.bv[level]] + df[self.av[level]])
        name = f'wap{level}'
        if cross: name += '_c'
        return wap.rename(name)

    def calc_mid_price(self, df):
        return (df[self.ap[1]] + df[self.bp[1]]).rename('mid_price') / 2

    def calc_spread(self, df):
        return (df[self.ap[1]] - df[self.bp[1]]).rename('spread')

    def calc_relative_spread(self, df):
        return (self.calc_spread(df) / self.calc_mid_price(df)).rename('relative_spread')

    def calc_price_breadth(self, df):
        """

        :param df:
        :return:

        .. [#] High-Frequency TradingAspects of market liquidityï¼ˆBervasï¼Œ2006ï¼‰
        """

        pb_b = (df[self.curr] - df[self.bp[1]]).rename('price_breadth_b')
        pb_a = (df[self.ap[1]] - df[self.curr]).rename('price_breadth_a')
        return pb_b, pb_a

    def calc_length_imbalance(self, df, level):
        """
        quantity imbalance

        :return:

        .. [#] Cao, C., Hansch, O., & Wang, X. (2009). The information content of an open limitâ€order book. Journal of Futures Markets: Futures, Options, and Other Derivative Products, 29(1), 16-41. ç¬¬30é¡µ
        """
        QR_level = (df[self.av[level]] - df[self.bv[level]]) / (df[self.av[level]] + df[self.bv[level]])
        QR_level = pd.Series(QR_level, index=df.index, name=f"li_{level}")
        return QR_level

    def calc_height_imbalance(self, df, level):
        """
        å’ŒåŸæ–‡æœ‰å‡ºå…¥ï¼ŒæŒ‰ç…§åŸæ–‡å…¬å¼å¯èƒ½ä¼šå‡ºç°infï¼Œå› ä¸ºåˆ†æ¯ä¸º0.æ ¹æ®æ–‡ç« æ‰€è¡¨è¾¾å«ä¹‰æˆ‘è¿›è¡Œäº†bid priceè®¡ç®—éƒ¨åˆ†çš„ä¿®æ”¹

        :return:

        .. [#] Cao, C., Hansch, O., & Wang, X. (2009). The information content of an open limitâ€order book. Journal of Futures Markets: Futures, Options, and Other Derivative Products, 29(1), 16-41. ç¬¬30é¡µ
        """
        assert level >= 2
        # åŸæ–‡
        # nominator = (df[self.ap[level]] - df[self.ap[level - 1]]) - (df[self.bp[level]] - df[self.bp[level - 1]])
        # denominator = (df[self.ap[level]] - df[self.ap[level - 1]]) + (df[self.bp[level]] - df[self.bp[level - 1]])
        # ä¿®æ”¹
        nominator = (df[self.ap[level]] - df[self.ap[level - 1]]) - (df[self.bp[level - 1]] - df[self.bp[level]])
        denominator = (df[self.ap[level]] - df[self.ap[level - 1]]) + (df[self.bp[level - 1]] - df[self.bp[level]])
        HR_level = pd.Series(nominator / denominator, index=df.index, name=f"hi_{level}")
        return HR_level

    def calc_spread_tick(self, df, num_levels=5):
        """
        ç›˜å£ä¿¡æ¯æ˜¯åˆ¤æ–­ä¸ªè‚¡çŸ­æœŸèµ°åŠ¿çš„é‡è¦ä¾æ®ï¼Œ
        çŸ­æœŸçš„å¸‚åœºä»·æ ¼æ˜¯å–å†³äºå½“å‰ ä¹°ç›˜ éœ€æ±‚é‡
        å’Œå–ç›˜ ä¾›ç»™ é‡æ„å»ºçš„å‡è¡¡ä»·æ ¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ¤æ–­ ç›˜å£ ä¹°å–æŒ‚å•çš„ç›¸å¯¹å¼ºå¼±å¯¹äºè‚¡ç¥¨ä»·æ ¼çš„
        çŸ­æœŸèµ°åŠ¿å…·æœ‰ç»Ÿè®¡æ„ä¹‰ä¸Šæ˜¾è‘—çš„é¢„åˆ¤ä½œç”¨ã€‚ å½“éœ€æ±‚è¿œå¤§äºä¾›ç»™æ—¶ï¼Œå‡è¡¡ä»·æ ¼å°†ä¸Šç§»ï¼› ç›¸å
        åœ°ï¼Œ å½“ä¾›ç»™è¿œå¤§äºéœ€æ±‚æ—¶ï¼Œå‡è¡¡ä»·æ ¼å°†ä¸‹é™ã€‚

        .. math::
            bid=\\Sigma_{i=1}^{num\\_levels}bidprice_i*bidvol_i*w_i\\newline
            ask=\\Sigma_{i=1}^{num\\_levels}askprice_i*askvol_i*w_i\\newline
            w_i=1-\\frac{i-1}{num\\_levels}\\newline
            spread\\_tick=(bid-ask)/(bid+ask)

        .. [#] 2019-09-05_å¤©é£è¯åˆ¸_å¸‚åœºå¾®è§‚ç»“æ„æ¢æç³»åˆ—ä¹‹äºŒï¼šè®¢å•ç°¿ä¸Šçš„alpha

        :param num_levels: æ¡£ä½æ•°é‡
        :return:
        """
        weights = {}
        for i in range(1, num_levels + 1):
            weights[i] = 1 - (i - 1) / num_levels

        bid = ask = 0
        for i in range(1, num_levels + 1):
            bid += df[self.bp[i]] * df[self.bv[i]] * weights[i]
            ask += df[self.ap[i]] * df[self.av[i]] * weights[i]

        spread_tick = (bid - ask) / (bid + ask)
        spread_tick = spread_tick.rename('spread_tick')
        return spread_tick

    def calc_realized_volatility(self, series):  # testit
        cum_vol = np.cumsum((series + 1) ** 2)
        denominator = np.arange(1, len(series) + 1, 1)
        realized_vol = (cum_vol / denominator - 1).rename('realized_vol')
        return realized_vol

    def calc_volume_order_imbalance(self, df: pd.DataFrame):
        """
        .. [#] Reference From <Order imbalance Based Strategy in High Frequency Trading>
        """
        current_bid_price = df[self.bp[1]]
        bid_price_diff = current_bid_price - current_bid_price.shift()
        current_bid_vol = df[self.bv[1]]
        bvol_diff = current_bid_vol - current_bid_vol.shift()
        # å³ç»“åˆï¼Œæœ€åä¸€ä¸ªwhereæ˜¯ç›´æ¥returnæ–°çš„arrayï¼Œè€Œä¸æ˜¯å»ä¿®æ”¹bid_price_diff
        bid_increment = np.where(bid_price_diff > 0, current_bid_vol,
                                 np.where(bid_price_diff < 0, 0,
                                          np.where(bid_price_diff == 0, bvol_diff, bid_price_diff)))

        current_ask_price = df[self.ap[1]]
        ask_price_diff = current_ask_price - current_ask_price.shift()
        current_ask_vol = df[self.av[1]]
        avol_diff = current_ask_vol - current_ask_vol.shift()
        ask_increment = np.where(ask_price_diff < 0, current_ask_vol,
                                 np.where(ask_price_diff > 0, 0,
                                          np.where(ask_price_diff == 0, avol_diff, ask_price_diff)))

        res = pd.Series(bid_increment - ask_increment, index=df.index, name='voi')

        return res

    def calc_buy_sell_pressure(self, df, level1, level2, method='MID'):
        """
        åˆ©ç”¨mid priceè®¡ç®—ä¹°å–å‹åŠ›å·®
        :param df:
        :param level1:
        :param level2:
        :param method:
        :return:

        .. [#] [å¤©é£è¯åˆ¸_ä¹°å–å‹åŠ›å¤±è¡¡â€”â€”åˆ©ç”¨é«˜é¢‘æ•°æ®æ‹“å±•ç›˜å£æ•°æ®](https://bigquant.com/wiki/pdfjs/web/viewer.html?file=/wiki/static/upload/2b/2bc961b3-e365-4afb-aa98-e9f9d9fa299e.pdf)
        """
        assert level1 < level2
        levels = np.arange(level1, level2 + 1)
        if method == "MID":
            M = self.calc_mid_price(df)
        else:
            raise NotImplementedError('price_weighted_pressure')

        # éœ€è¦æ³¨æ„ï¼Œå¦‚æœä¸é€‚ç”¨middle priceé‚£ä¹ˆå¯èƒ½åˆ†æ¯ä¼šå‡ºç°nan
        bid_d = np.array([M / (M - df[self.bp[s]]) for s in levels])
        # bid_d = [_.replace(np.inf,0) for _ in bid_d]
        bid_denominator = sum(bid_d)
        bid_weights = bid_d / bid_denominator
        press_buy = sum([df[self.bv[i + 1]] * w for i, w in enumerate(bid_weights)])

        ask_d = np.array([M / (df[self.ap[s]] - M) for s in levels])
        # ask_d = [_.replace(np.inf,0) for _ in ask_d]
        ask_denominator = sum(ask_d)
        ask_weights = ask_d / ask_denominator
        press_sell = sum([df[self.av[i + 1]] * w for i, w in enumerate(ask_weights)])

        res = pd.Series((np.log(press_buy) - np.log(press_sell)).replace([-np.inf, np.inf], np.nan),
                        name='buy_sell_pressure')
        return res

    def calc_gaps(self, df, level=5):
        """

        :param df:
        :return:

        .. [#] Price jump prediction in Limit Order Book. Ban Zheng, Eric Moulines, Fr Ìed Ìeric Abergel https://arxiv.org/pdf/1204.1381.pdf
        """

        def _meta_price_gap(df, i, side: str):
            """price gap"""
            res = None
            if side == 'b':
                res = df[self.bp[i + 1]] - df[self.bp[i]]
            elif side == 'a':
                res = df[self.ap[i + 1]] - df[self.ap[i]]
            return res.rename(f"{side}{i + 1}{i}_p_gap")

        # res = pd.DataFrame()
        l = []
        for i in range(level - 1, 0, -1):
            gap = _meta_price_gap(df, i, 'a')
            l.append(gap)
        for i in range(1, level):
            gap = _meta_price_gap(df, i, 'b')
            l.append(gap)
        res = pd.concat(l, axis=1)
        return res

    # def calc_event_dummies(self, trade_details,order_details):
    #     """
    #     .. [#] Price jump prediction in Limit Order Book. Ban Zheng, Eric Moulines, Fr Ìed Ìeric Abergel https://arxiv.org/pdf/1204.1381.pdf
    #
    #     :param events: a Series of integers indicating the type of events
    #     :param df:
    #     :return: one-hot dummies
    #
    #     """
    #     vs = ["BLO", "ALO", "BMO", "AMO", "BTT", "ATT"]
    #     mapper = {k+1: vs[k] for k in range(6)}
    #     event_df = pd.get_dummies(events.apply(lambda x: mapper[x]))
    #     return event_df

    def calc_bid_ask_volume_ratio(self, df: pd.DataFrame, level=5):
        """

        :param df:
        :param level:
        :return:

        .. [x] Price jump prediction in Limit Order Book. Ban Zheng, Eric Moulines,FrÂ´edÂ´eric Abergel
        """
        df1 = deepcopy(df)
        cols = [self.av[i] for i in range(1, level + 1)] + [self.bv[i] for i in range(1, level + 1)]
        # df1.loc[:,cols] = np.exp(df1[cols])

        bavr = 1
        res = pd.DataFrame()
        for i in range(1, level + 1):
            bavr *= np.clip(df1[self.bv[i]] / df1[self.av[i]], a_min=-32,
                            a_max=32)  # limit the value between the range of float32
            W_i = pd.Series(bavr, name=f'bid_ask_volume_ratio{i}')
            res = pd.concat([res, W_i], axis=1)

        return res

    def calc_window_avg_order_amount(self):
        """
        todo ä¸€æ®µæ—¶é—´çª—å£å†…çš„å¹³å‡è®¢å•ç¬”æ•°
        :return:
        """
        pass

    def calc_window_avg_trade_amount(self):
        """
        todo ä¸€æ®µæ—¶é—´çª—å£å†…çš„å¹³å‡æˆäº¤ç¬”æ•°
        :return:
        """
        pass

    def calc_window_avg_order_vol(self):
        """
        todo ä¸€æ®µæ—¶é—´çª—å£å†…çš„å¹³å‡è®¢å•é‡
        :return:
        """
        pass

    def calc_window_avg_trade_vol(self):
        """
        todo ä¸€æ®µæ—¶é—´çª—å£å†…çš„å¹³å‡æˆäº¤é‡
        :return:
        """
        pass

    def calc_window_weighted_avg_order_vol(self):
        """
        todo ä¸€æ®µæ—¶é—´çª—å£å†…çš„åŠ æƒå¹³å‡è®¢å•é‡
        :return:
        """
        pass

    def calc_window_weighted_avg_trade_vol(self):
        """
        todo ä¸€æ®µæ—¶é—´çª—å£å†…çš„åŠ æƒå¹³å‡æˆäº¤é‡
        :return:
        """
        pass

    def calc_momentum(self):
        """
        todo è®¡ç®—ä¸€äº›åŠ¨é‡å› å­
        :return:
        """
        pass

    def generate_cross_section(self, clean_obh, level=5):
        """
        è®¡ç®—æˆªé¢featuresï¼Œä¸åŒ…å«ä¸åŒæ—¶åˆ»æ•°æ®æ‰€æ„å»ºçš„å› å­
        :param clean_obh:
        :param level:
        :return:
        """

        self.mp = self.calc_mid_price(clean_obh)
        # todo å¥½åƒæ²¡å•¥ç”¨ï¼Œå› ä¸ºåœ¨10msçš„æ•°æ®ä¸­ï¼Œè¯¥åˆ—ç»å¤§éƒ¨åˆ†éƒ½æ˜¯0ã€‚ä½†å¥½åƒå¦‚æœè¿™ä¹ˆçœ‹ï¼Œé‚£ä¹ˆæ‰€æœ‰å› å­éƒ½æ˜¯ç¨€ç–çš„ï¼Œå› ä¸ºdiffåå¤§éƒ¨åˆ†æ—¶é—´éƒ½æ˜¯0
        self.spread = self.calc_spread(clean_obh)
        self.breadth_b, self.breadth_a = self.calc_price_breadth(clean_obh)
        self.rs = self.calc_relative_spread(clean_obh)
        self.st = self.calc_spread_tick(clean_obh, num_levels=level)
        self.voi = self.calc_volume_order_imbalance(clean_obh)
        self.bsp = self.calc_buy_sell_pressure(clean_obh, level1=1, level2=level, method='MID')
        # self.volatility = self.calc_realized_volatility(self.mp_ret)  # è®¡ç®—realized volatilityéœ€è¦åœ¨ä¹‹åçš„rolling aggé˜¶æ®µ
        self.gaps = self.calc_gaps(clean_obh, level=level)
        self.bavr = self.calc_bid_ask_volume_ratio(clean_obh, level=level)

        self.waps = [self.calc_wap(clean_obh, level=i, cross=True) for i in range(1, level + 1)] + [
            self.calc_wap(clean_obh, level=i, cross=False) for i in range(1, level + 1)]
        self.lis = [self.calc_length_imbalance(clean_obh, level=i) for i in range(1, level)]
        self.his = [self.calc_height_imbalance(clean_obh, level=i) for i in range(2, level)]

        self.mp_ret = np.log(self.mp).diff().rename("mid_p_ret")
        self.wap1_ret = np.log(self.waps[0]).diff().rename("wap1_ret")
        self.wap2_ret = np.log(self.waps[1]).diff().rename("wap2_ret")

        self.features = pd.concat(
            self.waps
            + self.lis
            + self.his
            + [self.mp,
               self.spread,
               self.breadth_b,
               self.breadth_a,
               self.rs,
               self.st,
               self.voi,
               self.bsp,
               # self.volatility, # ä¼šé€æ¸å‡å°ä¸º0
               self.gaps,
               self.bavr,
               self.mp_ret,
               self.wap1_ret,
               self.wap2_ret,
               ],
            axis=1)
        # fixmeå°†2 level headerè½¬ä¸º1 level
        # df_feature.columns = ['_'.join(col) for col in df_feature.columns]  # time_id is changed to time_id_

        return self.features

    # def agg_features(self, features: pd.DataFrame, agg_freq: str):
    #     features = features.resample(agg_freq).agg([np.mean, np.std, np.median])
    #     return features


if __name__ == '__main__':
    pass
