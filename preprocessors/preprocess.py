# -*- coding=utf-8 -*-
# @File     : preprocess.py
# @Time     : 2023/8/2 18:32
# @Author   : EvanHong
# @Email    : 939778128@qq.com
# @Project  : 2023.06.08è¶…é«˜é¢‘ä¸Šè¯50æŒ‡æ•°è®¡ç®—
# @Description: éœ€è¦ä¿è¯è¯¥æ–‡ä»¶cleanï¼Œä¸è¦åŠ å…¥è¿‡å¤šæ— ç”¨ç®—æ³•
import logging
import re
from copy import deepcopy
from datetime import timedelta

import numpy as np
import pandas as pd
from numba import jit
from sklearn.preprocessing import StandardScaler

from backtest.config import *
from backtest import config
from backtest.support import *
from backtest.support import update_date
import pickle
from abc import abstractmethod
from typing import Union, List


class BasePreprocessor(object):
    def __init__(self, *args, **kwargs):
        self.args = args
        for key, value in kwargs.items():
            self.__setattr__(key, value)


class BaseDataPreprocessor(BasePreprocessor):
    """
    ä¸»è¦ç”¨äºå¯¹ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥é¢„å¤„ç†ï¼Œä»è€Œé€‚åº”å„ä¸ªæ¨¡å‹
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scaler = StandardScaler()

    def std_scale(self, data, refit=True, *args):
        res = []

        def _meta(data: pd.DataFrame, scaler):
            cols = data.columns
            index = data.index
            return pd.DataFrame(scaler.transform(data), index=index, columns=cols)

        if refit: self.scaler.fit(data)
        data = _meta(data, self.scaler)
        res.append(data)
        for other in args:
            other = _meta(other, self.scaler)
            res.append(other)
        if len(res) == 1:
            return res[0]
        return tuple(res)

    def sub_illegal_punctuation(self, string):
        return re.sub('\W+', '_', string)

    def save_scaler(self, dir, file_name):
        # file_name=re.sub('\W+','_', file_name)
        with open(dir + file_name, 'wb') as f:
            pickle.dump(self.scaler, f)

    def load_scaler(self, dir, file_name):
        # file_name=re.sub('\W+','_', file_name)
        with open(dir + file_name, 'rb') as f:
            self.scaler = pickle.load(f)

    @classmethod
    def get_flattened_Xy(cls, *args, **kwargs):
        pass

    @abstractmethod
    def get_stacked_Xy(self, *args, **kwargs):
        """
        ä¾‹å¦‚5è¡ŒXå¯¹åº”1è¡Œy
        Parameters
        ----------
        args
        kwargs

        Returns
        -------

        """
        pass

    @abstractmethod
    def align_Xy(self, *args, **kwargs):
        pass


class ShiftDataPreprocessor(BaseDataPreprocessor):
    """
    å¯¹featureè¿›è¡Œå¹³ç§»æ‹¼æ¥ï¼Œç”Ÿæˆå¯è¾“å…¥æ¨¡å‹çš„æ•°æ®

    NOTE
    ----
        å¾ˆå¯èƒ½å¯¼è‡´ç»´åº¦çˆ†ç‚¸ï¼Œå› ä¸ºå¢åŠ ä½¿ç”¨çš„æ•°æ®ç‚¹çš„æ•°é‡ï¼Œä¼šä½¿å¾—æœ€ç»ˆç‰¹å¾æ•°å‘ˆçº¿æ€§å€æ•°å¢é•¿

    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @staticmethod
    def shift_X(X, use_n_steps):
        XX = pd.concat(
            [X.shift(i).rename(columns={col: col + f'_lag{i}' for col in X.columns}) for i in
             range(use_n_steps)],
            axis=1)
        return XX

    @classmethod
    def get_flattened_Xy(cls, alldatas, num, target, pred_n_steps, use_n_steps, drop_current):
        if target == Target.ret.name:
            current = alldatas[num]['current']
            y = np.log(current.shift(-pred_n_steps) / current)
            y = y.rename(f'{target}_{pred_n_steps * 0.2}s')
        elif target == Target.mid_p_ret.name:
            mid_price = alldatas[num]['mid_price']
            y = np.log(mid_price.shift(-pred_n_steps) / mid_price)
            y = y.rename(f'{target}_{pred_n_steps * 0.2}s')
        else:
            raise NotImplementedError("align_Xy target not defined")

        if drop_current:
            X = alldatas[num].drop(columns=['current'])
        else:
            X = alldatas[num]

        X = cls.shift_X(X, use_n_steps)

        # x yå·²ç»å¯¹é½ï¼Œåªéœ€è¦è¿›è¡ŒåŒæ ·çš„è£å‰ªï¼Œå»æ‰nanå°±å¯ä»¥
        y = y.iloc[use_n_steps:-pred_n_steps]
        X = X.iloc[use_n_steps:-pred_n_steps]

        return X, y


class AggDataPreprocessor(BaseDataPreprocessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @staticmethod
    def calc_realized_volatility(series: pd.Series, **kwargs):
        """annualized realized volatility
        realized volatilityå’Œretåºåˆ—çš„stdä¸ä¸€æ ·åœ¨äºï¼Œstdæ˜¯å‡å»retçš„å‡å€¼ï¼Œè€Œrealized volatilityå¯ä»¥çœ‹åšæ˜¯å‡å»0

        Notes
        -----
        åˆ†æ¯é‡‡ç”¨æ ·æœ¬å¤§å°-1ï¼Œå³æ ·æœ¬realized volatility

        Parameters
        ----------
        series

        Returns
        -------

        References
        ----------
        [1] https://www.realvol.com/VolFormula.htm
        [2] https://vinsight.shnyu.edu.cn/about_volatility.php
        """
        # assert (series>=0).values.all() # å·²åœ¨å¤–éƒ¨ä»£ç ä¿è¯è¿™ä¸€ç‚¹ï¼Œå‡ºäºæ•ˆç‡è€ƒè™‘ï¼Œæš‚æ—¶æ³¨é‡Šè¯¥è¡Œ

        # if kwargs.get('n') is None:
        #     # freq = series.index.freq
        #     logging.warning('AggDataPreprocessor.calc_realized_volatility: freq is none')
        #     freq=(series.index[1:] - series.index[:-1]).median()
        #     n=252*4*60*60/freq.seconds
        # else:
        #     n=kwargs['n']
        n = 1
        temp = series.dropna()
        res = pd.Series(data=[np.sqrt(np.matmul(temp.T, temp) / (len(temp) - 1) * n)], index=[series.index[-1]])
        return res

    @staticmethod
    def realized_volatility(series: pd.Series):
        """RSS, root sum of squares?"""
        raise DeprecationWarning('realized_volatility')
        return np.sqrt(np.sum(series ** 2))

    @staticmethod
    def diff(series: pd.Series):
        return series.iloc[-1] - series.iloc[0]

    @staticmethod
    def _calc_decay_weight(m, H):
        """è®¡ç®—è¡°å‡åŠ æƒæƒé‡
        è€ƒè™‘åˆ°è·ç¦»å½“å‰æ—¶é—´è¾ƒè¿œæ—¥çš„æ•°æ®å¯¹äºå½“å‰å› å­å–å€¼çš„å½±å“åº”è¯¥æ›´å°ï¼Œè€Œè¿‘æœŸæ•°æ®çš„å½±å“åº”è¯¥æ›´å¤§ï¼Œå› æ­¤æˆ‘ä»¬åœ¨åŠ æƒæ”¶ç›Šä¸Šè¿›è¡Œæ—¶é—´åŠè¡°çš„æƒé‡å€¾æ–œ

        Parameters
        ----------
        m:
            æ”¶ç›ŠåŠ æƒçš„çª—å£é•¿åº¦
        H:
            ğ» ä¸ºåŠè¡°æœŸ

        Returns
        -------

        References
        ----------
        [1] 2023-07-14_ä¸œæ–¹è¯åˆ¸_å› å­é€‰è‚¡ç³»åˆ—ä¹‹ä¹åå››ï¼šUMR2.0ï¼Œé£é™©æº¢ä»·è§†è§’ä¸‹çš„åŠ¨é‡åè½¬ç»Ÿä¸€æ¡†æ¶å†å‡çº§.pdf
        """
        weights = np.array([np.power(2, -(m - j + 1) / H) for j in range(1, m + 1)])
        weights = weights / sum(weights)
        return weights

    @classmethod
    def momentum_UMR(cls, features: pd.DataFrame):
        """å¯¹åŸç®—æ³•è¿›è¡Œäº†ç®€åŒ–ï¼Œå»æ‰äº†returnçš„éƒ¨åˆ†

        Parameters
        ----------
        features:
            æ‰€æœ‰ç‰¹å¾

        split:
            åˆ†ä¸ºnç­‰åˆ†ï¼Œè®¡ç®—é£é™©è°ƒæ•´åŠ æƒæ”¶ç›Š

        Returns
        -------

        References
        ----------
        [1] 2023-07-14_ä¸œæ–¹è¯åˆ¸_å› å­é€‰è‚¡ç³»åˆ—ä¹‹ä¹åå››ï¼šUMR2.0ï¼Œé£é™©æº¢ä»·è§†è§’ä¸‹çš„åŠ¨é‡åè½¬ç»Ÿä¸€æ¡†æ¶å†å‡çº§.pdf
        """
        if len(features) == 0: return pd.DataFrame({
            'risk_vol_avg': [np.nan],
            'risk_vol_std': [np.nan],
            'risk_ret_std': [np.nan],
            'risk_ret_skew': [np.nan],
        })
        split = 5  # åˆ†ä¸ºnç­‰åˆ†ï¼Œè®¡ç®—é£é™©è°ƒæ•´åŠ æƒæ”¶ç›Š
        H = 2
        agg_rows = step_rows = int(len(features) / split)
        weights = cls._calc_decay_weight(split - 1, H=H)  # æœ€åä¸€ä¸ªframeä¸éœ€è¦åŠ æƒ
        # _features = features.iloc[::step_rows]
        # returns = _features['wap1_ret']
        # è®¡ç®—é£é™©ç³»æ•°riskï¼Œä¸åŒä»£ç†å˜é‡æœ‰ä¸åŒè®¡ç®—æ–¹æ³•
        # risk_vol_avg = features['volume'].rolling(agg_rows, min_periods=agg_rows, step=step_rows).mean() - \
        #                _features['volume']
        # risk_ret_std = (features['wap1_ret']*100000).rolling(agg_rows, min_periods=agg_rows, step=step_rows).std() - \
        #                (_features['wap1_ret']*100000)
        # risk_ret_skew = features['wap1_ret'].rolling(agg_rows, min_periods=agg_rows, step=step_rows).skew() - \
        #                 _features['wap1_ret']
        risk_vol_avg = features['volume'].rolling(agg_rows, min_periods=agg_rows, step=step_rows).mean()
        risk_vol_std = features['volume'].rolling(agg_rows, min_periods=agg_rows, step=step_rows).std()
        risk_ret_std = (features['wap1_ret'] * 100000).rolling(agg_rows, min_periods=agg_rows, step=step_rows).mean()
        risk_ret_skew = (features['wap1_ret'] * 100000).rolling(agg_rows, min_periods=agg_rows, step=step_rows).skew()

        # å¯¹åŸç®—æ³•è¿›è¡Œäº†ç®€åŒ–ï¼Œå»æ‰äº†returnçš„éƒ¨åˆ†
        UMR = pd.DataFrame({
            'risk_vol_avg': [np.sum(weights * risk_vol_avg.iloc[:-1]) - risk_vol_avg.iloc[-1]],
            'risk_vol_std': [np.sum(weights * risk_vol_std.iloc[:-1]) - risk_vol_std.iloc[-1]],
            'risk_ret_std': [np.sum(weights * risk_ret_std.iloc[:-1]) - risk_ret_std.iloc[-1]],
            'risk_ret_skew': [np.sum(weights * risk_ret_skew.iloc[:-1]) - risk_ret_skew.iloc[-1]],
        })
        return UMR

    def agg_features(self, features: pd.DataFrame, use_events=True):
        """
        ç”¨äºå°†éå¸¸é«˜é¢‘çš„æ•°æ®aggä¸ºé«˜åº¦æŠ½è±¡çš„ç‰¹å¾ï¼Œå¦‚meanã€stdã€realized volç­‰ï¼Œå¹¶ä¸”å¯¹aggçš„å¤§å°ä¸åŒå¯ä»¥æ„é€ å‡ºåŠ¨é‡ç‰¹å¾
        Parameters
        ----------
        features
            ['ALO','AMO','ATT','BLO','BMO','BTT','a1_p','a1_v','a21_p_gap','a2_p','a2_v','a32_p_gap','a3_p','a3_v','a43_p_gap','a4_p','a4_v','a54_p_gap','a5_p','a5_v','b1_p','b1_v','b21_p_gap','b2_p','b2_v','b32_p_gap','b3_p','b3_v','b43_p_gap','b4_p','b4_v','b54_p_gap','b5_p','b5_v','bid_ask_volume_ratio1','bid_ask_volume_ratio2','bid_ask_volume_ratio3','bid_ask_volume_ratio4','bid_ask_volume_ratio5','buy_sell_pressure','cum_turnover','cum_vol','current','hi_2','hi_3','hi_4','li_1','li_2','li_3','li_4','mid_price','price_breadth_a','price_breadth_b','relative_spread','spread','spread_tick','turnover','voi','volume','wap1','wap1_c','wap2','wap2_c','wap3','wap3_c','wap4','wap4_c','wap5','wap5_c']
        Returns
        -------

        """

        agg_rows = int(agg_timedelta / min_timedelta)
        step_rows = int(pred_timedelta / min_timedelta)

        agg_mapper = {k: [np.mean, np.std] for k in features.columns}
        agg_diff = {k: [self.diff] for k in ['cum_turnover', 'cum_vol']}
        agg_rv = {k: [self.calc_realized_volatility] for k in ['wap1_ret', 'wap2_ret', 'mid_p_ret']}
        agg_sum = {k: [np.sum] for k in ['ALO', 'AMO', 'ATT', 'BLO', 'BMO', 'BTT']}
        agg_mapper.update(agg_diff)
        agg_mapper.update(agg_rv)
        if use_events:
            agg_mapper.update(agg_sum)

        # åŠ å…¥realized volå› å­ï¼Œéœ€è¦æ³¨æ„è¯¥dfæ˜¯2 level header
        last_n_rows = [agg_rows, int(agg_rows / 2)]
        # last_n_rows = [agg_rows]
        dfs = []
        for last_n in last_n_rows:
            agg_features = features.rolling(last_n, min_periods=last_n, step=step_rows, closed='left',
                                            center=False).agg(agg_mapper)  # åˆ é™¤äº†median
            agg_features.columns = ['_'.join(col) for col in agg_features.columns]

            mom_features = pd.concat([self.momentum_UMR(table) for table in
                                      features.rolling(last_n, min_periods=last_n, step=step_rows, closed='left',
                                                       center=False,
                                                       method='table')])

            mom_features.index = agg_features.index
            temp = pd.concat([agg_features, mom_features], axis=1)
            temp.columns = [col + '_' + str(last_n) for col in temp.columns]
            dfs.append(temp)

        agg_features = pd.concat(dfs, axis=1)

        return agg_features

    @classmethod
    def get_flattened_Xy(cls, alldatas, num, target, pred_n_steps, use_n_steps, drop_current):
        raise NotImplementedError

    def align_Xy(self, X, y, pred_timedelta):
        start_time = X.index
        tar_time = start_time + pred_timedelta

        available_time = [True if t in y.index else False for t in tar_time]
        start_time = start_time[available_time]
        tar_time = tar_time[available_time]

        X = X.loc[start_time]
        y = y.loc[tar_time]

        return X, y


class LobCleanObhPreprocessor(BasePreprocessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @staticmethod
    def split_price(order_book_history: pd.DataFrame, window_size=-1):
        """
        å°†order_book_historyåˆ†è§£ä¸ºpriceå’Œvolumeçš„ä¸¤ä¸ª2d dataframes
        :param window_size: å‡ æ¡£ç›˜å£ä¿¡æ¯ï¼Œ10ä»£è¡¨ä¹°å–å„10æ¡£
        :param order_book_history:
        :return:
        """

        def meta(row: pd.Series):
            return row.dropna().index.tolist()

        temp = pd.DataFrame(order_book_history.apply(lambda x: meta(x), axis=1, result_type="expand"))
        origin_level = int(temp.shape[1] / 2)
        temp.columns = [f'a{i}_p' for i in range(origin_level, 0, -1)] + [f'b{i}_p' for i in range(1, origin_level + 1)]
        if window_size == -1:
            window_size = origin_level

        levels = [f'a{i}_p' for i in range(window_size, 0, -1)] + [f'b{i}_p' for i in range(1, window_size + 1)]

        temp = temp.loc[:, levels]
        temp.index = order_book_history.index
        return temp

    @staticmethod
    def split_volume(order_book_history: pd.DataFrame, window_size=-1):
        """
        å°†order_book_historyåˆ†è§£ä¸ºpriceå’Œvolumeçš„ä¸¤ä¸ª2d dataframes
        :param window_size: å‡ æ¡£ç›˜å£ä¿¡æ¯ï¼Œ10ä»£è¡¨ä¹°å–å„10æ¡£
        :param order_book_history:
        :return:
        """

        def _meta(row: pd.Series):
            return row.dropna().values

        temp = pd.DataFrame(order_book_history.apply(lambda x: _meta(x), axis=1, result_type="expand"))
        origin_level = int(temp.shape[1] / 2)
        temp.columns = [f'a{i}_v' for i in range(origin_level, 0, -1)] + [f'b{i}_v' for i in range(1, origin_level + 1)]
        if window_size == -1:
            window_size = origin_level
        levels = [f'a{i}_v' for i in range(window_size, 0, -1)] + [f'b{i}_v' for i in range(1, window_size + 1)]
        temp = temp.loc[:, levels]
        temp.columns = levels
        temp.index = order_book_history.index
        return temp

    @staticmethod
    def split_volume_price(order_book_history_dict: dict, window_size=-1):
        """
        å°†order_book_history_dictåˆ†è§£ä¸ºvol_df,price_df
        Parameters
        ----------
        order_book_history_dict
        window_size

        Returns
        -------

        """
        if window_size == -1:
            window_size = len(list(order_book_history_dict.values())[0])
        volumes = [f'a{i}_v' for i in range(window_size, 0, -1)] + [f'b{i}_v' for i in range(1, window_size + 1)]
        prices = [f'a{i}_p' for i in range(window_size, 0, -1)] + [f'b{i}_p' for i in range(1, window_size + 1)]
        # order_book_history_dict={k: {for kk in order_book_history_dict[k].keys()} for k in order_book_history_dict.items()}
        vol_dict = {k: [vv for vv in v.values()] for k, v in order_book_history_dict.items()}
        price_dict = {k: [vv for vv in v.keys()] for k, v in order_book_history_dict.items()}
        vol_df = pd.DataFrame.from_dict(vol_dict, orient='index')
        price_df = pd.DataFrame.from_dict(price_dict, orient='index')
        # è½¬ç½®æˆä»·æ ¼ä»é«˜åˆ°ä½
        vol_df = vol_df.loc[:, vol_df.columns[::-1]]
        price_df = price_df.loc[:, price_df.columns[::-1]]
        vol_df.columns = volumes
        price_df.columns = prices
        return vol_df, price_df

    # @staticmethod
    # def _gen_clean_obh(datafeed, snapshot_window):
    #     """
    #
    #     :return:
    #     """
    #     # order_book_history = datafeed.order_book_history
    #     # assert len(order_book_history) > 0
    #     # obh = order_book_history
    #     # try:
    #     #     obh.index = pd.to_datetime(obh.index)
    #     # except:
    #     #     obh = obh.T
    #     #     obh.index = pd.to_datetime(obh.index)
    #     # obh = obh.sort_index(ascending=True)
    #     # obh.columns = obh.columns.astype(float)
    #     # obh_v = LobCleanObhPreprocessor.split_volume(obh, window_size=snapshot_window)
    #     # obh_p = LobCleanObhPreprocessor.split_price(obh, window_size=snapshot_window)
    #     obh_p,obh_v=LobCleanObhPreprocessor.split_volume_price()
    #     current = datafeed.current
    #     mid_p = (obh_p[str(LobColTemplate('a', 1, 'p'))] + obh_p[str(LobColTemplate('a', 1, 'p'))]).rename('mid_p') / 2
    #
    #     clean_obh = pd.concat([obh_p, obh_v, current, mid_p], axis=1).ffill().bfill()
    #     clean_obh.index = pd.to_datetime(clean_obh.index)
    #     clean_obh = LobTimePreprocessor.del_untrade_time(clean_obh, cut_tail=True)
    #     clean_obh = LobTimePreprocessor.add_head_tail(clean_obh,
    #                                                   head_timestamp=config.important_times[
    #                                                       'continues_auction_am_start'],
    #                                                   tail_timestamp=config.important_times['continues_auction_pm_end'])
    #
    #     return clean_obh

    # @staticmethod
    # def save_clean_obh(clean_obh, file_root, date, stk_name):
    #     clean_obh.to_csv(file_root + FILE_FMT_clean_obh.format(date, stk_name))
    #
    # @staticmethod
    # def gen_and_save(datafeed, save_root, date: str, stk_name: str, snapshot_window):
    #     clean_obh = LobCleanObhPreprocessor._gen_clean_obh(datafeed, snapshot_window)
    #     LobCleanObhPreprocessor.save_clean_obh(clean_obh, save_root, date, stk_name)

    def run_batch(self):
        """
        æ‰¹é‡å¤„ç†
        :return:
        """
        pass

def str2timedelta( time_str: str, multiplier: int = None) -> Optional[datetime.timedelta]:
    raise PendingDeprecationWarning("è¯¥å‡½æ•°å°†è¢«åºŸå¼ƒï¼Œè¯·æ”¹ç”¨`backtest.preprocessors.preprocess.GeneralTimePreprocessor`")
    if time_str is None:
        return timedelta(seconds=0)

    if multiplier is None: multiplier = 1
    time_str_list = time_str.split(' ')
    delta = timedelta(seconds=0)
    for time_str in time_str_list:
        if time_str.endswith('min'):
            td = timedelta(minutes=int(time_str[:-3]) * multiplier)
        elif time_str.endswith('m'):
            td = timedelta(minutes=int(time_str[:-1]) * multiplier)
        elif time_str.endswith('ms'):
            td = timedelta(milliseconds=int(time_str[:-2]) * multiplier)
        elif time_str.endswith('s'):
            td = timedelta(seconds=int(time_str[:-1]) * multiplier)
        else:
            raise NotImplementedError("in config")
        delta += td
    return delta

class GeneralTimePreprocessor(BasePreprocessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.daily_timedeltas = {
            'open_call_auction_start': timedelta(hours=9, minutes=15),
            'open_call_auction_end': timedelta(hours=9, minutes=25),
            'continues_auction_am_start': timedelta(hours=9, minutes=30),
            'continues_auction_am_end': timedelta(hours=11, minutes=30),
            'continues_auction_pm_start': timedelta(hours=13, minutes=00),
            'continues_auction_pm_end': timedelta(hours=14, minutes=57),
            'close_call_auction_start': timedelta(hours=14, minutes=57),
            'close_call_auction_end': timedelta(hours=15, minutes=00), }

    def str2timedelta(self, time_str: str, multiplier: int = None) -> Optional[datetime.timedelta]:
        if time_str is None:
            return timedelta(seconds=0)

        if multiplier is None: multiplier = 1
        time_str_list = time_str.split(' ')
        delta = timedelta(seconds=0)
        for time_str in time_str_list:
            if time_str.endswith('min'):
                td = timedelta(minutes=int(time_str[:-3]) * multiplier)
            elif time_str.endswith('m'):
                td = timedelta(minutes=int(time_str[:-1]) * multiplier)
            elif time_str.endswith('ms'):
                td = timedelta(milliseconds=int(time_str[:-2]) * multiplier)
            elif time_str.endswith('s'):
                td = timedelta(seconds=int(time_str[:-1]) * multiplier)
            else:
                raise NotImplementedError("in config")
            delta += td
        return delta

    def del_untrade_time_(self, df: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
                          cut_tail=True,
                          strip: list = None,
                          use_default=False
                          ):
        """

        Parameters
        ----------
        df :
        cut_tail :
        strip :
        use_default : bool,
            if trueï¼Œé‚£å°±ä½¿ç”¨é»˜è®¤çš„strip=['5m', '1m', '1m', '5m']

        Returns
        -------

        """
        if strip is None:
            strip = [None, None, None, None]
        if use_default:
            strip=['5m', '1m', '1m', '5m']
        if cut_tail and strip[3] is None:
            strip[3] = '3m'
        for i in range(len(strip)):
            if strip[i] is None:
                strip[i] = '0s'

        placeholder = '2000-01-01'  # ä»…ç”¨äºæ„å»ºdatetimeä»è€Œèƒ½å®ç°æ—¶é—´è¿ç®—ï¼Œ
        df.index = pd.to_datetime(df.index)
        df = df.sort_index()
        index = df.index.time.astype(str)
        idx = np.full(shape=(len(index)), fill_value=True, dtype=bool)
        idx1 = np.full(shape=(len(index)), fill_value=True, dtype=bool)
        # ä¸Šä¸‹åˆå¿…é¡»åˆ†å¼€ï¼Œå› ä¸ºä¸¤ä¸ªæ—¶é—´æ®µä¸èƒ½åŒæ—¶ç”¨logical_and
        # ä¸Šåˆ
        if strip[0] is not None:
            idx = np.logical_and(idx, index >= str(
                (pd.to_datetime(f"{placeholder} 09:30:00") + self.str2timedelta(strip[0])).time()))
        if strip[1] is not None:
            idx = np.logical_and(idx, index <= str(
                (pd.to_datetime(f"{placeholder} 11:30:00") - self.str2timedelta(strip[1])).time()))
        # ä¸‹åˆ
        if strip[2] is not None:
            idx1 = np.logical_and(idx1, index >= str(
                (pd.to_datetime(f"{placeholder} 13:00:00") + self.str2timedelta(strip[2])).time()))
        if strip[3] is not None:
            idx1 = np.logical_and(idx1, index <= str(
                (pd.to_datetime(f"{placeholder} 15:00:00") - self.str2timedelta(strip[3])).time()))
        idx = np.logical_or(idx, idx1)
        return df.loc[idx]

    def del_untrade_time(self, df: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]], cut_tail=True,
                         strip: str = None,
                         drop_last_row=False,
                         split_df=False,
                         pad_margin=False):
        """

        Parameters
        ----------
        drop_last_row :
            å»æ‰æœ€åä¸€è¡Œ
        df
        cut_tail
            å»æ‰å°¾ç›˜3minç«ä»·
        strip
            å»æ‰å¼€æ”¶ç›˜n minçš„æ•°æ®
        split_df
            æ˜¯å¦è¿”å›ä¸Šä¸‹åˆåˆ†å¼€çš„df
        pad_margin
            æ˜¯å¦è¦ffillå¡«å……ä¸Šä¸‹åˆå¼€æ”¶ç›˜çš„æ—¶åˆ»æ•°æ®ï¼ˆå¦‚å¡«å……09:30:00ã€11:30:00ä»è€Œä¿è¯ä¸Šåˆæ•°æ®åœ¨asfreqæˆ–æ˜¯resampleä¹‹åèƒ½å¤Ÿæ•´é½ï¼‰

        Returns
        -------

        """

        def _meta(df, cut_tail, strip, split_df, pad_margin):
            """
            æ‰€æœ‰æ•°æ®éµä»ffillçš„æ€è·¯
            """
            date_today = pd.to_datetime(df.index[0].date())

            morning_start_time = date_today + self.daily_timedeltas['continues_auction_am_start']
            morning_end_time = date_today + self.daily_timedeltas['continues_auction_am_end']
            afternoon_start_time = date_today + self.daily_timedeltas['continues_auction_pm_start']
            afternoon_end_time = date_today + self.daily_timedeltas[
                'close_call_auction_end'] if not cut_tail else date_today + self.daily_timedeltas[
                'continues_auction_pm_end']
            if strip is not None:
                strip_timedelta = str2timedelta(strip)
                morning_start_time = date_today + self.daily_timedeltas['continues_auction_am_start'] + strip_timedelta
                afternoon_end_time = min(date_today + self.daily_timedeltas['close_call_auction_end'] - strip_timedelta,
                                         afternoon_end_time)

            # åˆ¤æ–­å‡ ä¸ªç‰¹æ®Šæ—¶é—´ç‚¹æ˜¯å¦æœ‰æ•°æ®ï¼Œæ²¡æœ‰æ•°æ®åˆ™ç”¨æœ€è¿‘çš„æ•°æ®æ¥å¡«å……
            a = df.loc[morning_start_time:date_today + self.daily_timedeltas['continues_auction_am_end']]  # å·¦é—­å³é—­
            b = df.loc[date_today + self.daily_timedeltas['continues_auction_pm_start']:afternoon_end_time]  # å·¦é—­å³é—­
            if pad_margin:
                # ffillå¡«å……ä¸Šä¸‹åˆå¼€æ”¶ç›˜çš„æ—¶åˆ»æ•°æ®ï¼ˆå¦‚å¡«å……09: 30:00ã€11: 30:00ä»è€Œä¿è¯ä¸Šåˆæ•°æ®åœ¨asfreqæˆ–æ˜¯resampleä¹‹åèƒ½å¤Ÿæ•´é½ï¼‰
                pad_morning_start = df.loc[df.index < morning_start_time].iloc[-1].to_frame().T
                pad_morning_end = df.loc[df.index < morning_end_time].iloc[-1].to_frame().T
                pad_afternoon_start = df.loc[df.index < afternoon_start_time].iloc[-1].to_frame().T
                pad_afternoon_end = df.loc[df.index < afternoon_end_time].iloc[-1].to_frame().T
                for i, (pad, dt) in enumerate(
                        [(pad_morning_start, morning_start_time), (pad_morning_end, morning_end_time),
                         (pad_afternoon_start, afternoon_start_time), (pad_afternoon_end, afternoon_end_time)]):
                    if dt in df.index: continue
                    pad.index = pad.index.map(lambda x: dt)
                    if i < 2:
                        a = pd.concat([a, pad], axis=0)
                    elif i >= 2:
                        b = pd.concat([b, pad], axis=0)

            a = a.sort_index()
            b = b.sort_index()
            if split_df:
                if drop_last_row:
                    b = b.iloc[:-1]
                temp = (a, b)
            else:
                temp = pd.concat([a, b], axis=0)
                temp = temp.sort_index()
                if drop_last_row:
                    temp = temp.iloc[:-1]

            return temp

        if isinstance(df, list):
            return [_meta(_df, cut_tail=cut_tail, strip=strip, split_df=split_df, pad_margin=pad_margin) for _df in df]
        elif isinstance(df, pd.DataFrame) or isinstance(df, pd.Series):
            return _meta(df, cut_tail=cut_tail, strip=strip, split_df=split_df, pad_margin=pad_margin)


class LobTimePreprocessor(BasePreprocessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # self.freq = '200ms'
        raise PendingDeprecationWarning(
            "`LobTimePreprocessor`å°†è¢«æ·˜æ±°ï¼Œè¯·ä½¿ç”¨`backtest.preprocessors.preprocess.GeneralTimePreprocessor`")

    @staticmethod
    def del_middle_hours(df: Union[pd.Series, pd.DataFrame], morning_end: datetime.time = None,
                         afternoon_begin: datetime.time = None, margin='left'):
        """
        å»æ‰æ—¥å†…ä¸­é—´ä¸äº¤æ˜“çš„ä¸€æ®µæ—¶é—´ï¼Œå¹¶å¯ä»¥è®¾ç½®å‚æ•°ä½¿å¾—å¯ä»¥å»æ‰ä¸­åˆæ”¶ç›˜å‰ä¸€æ®µæ—¶é—´çš„ä¸å¯ä½¿ç”¨æ•°æ®
        Parameters
        ----------
        df :
        morning_end :
        afternoon_begin :
        margin :
            both å·¦é—­å³é—­
            left å·¦é—­å³å¼€
            right å·¦å¼€å³é—­
            none å·¦å¼€å³å¼€

        Returns
        -------

        """
        assert not (morning_end is None and afternoon_begin is None)
        if morning_end is None:
            return df.loc[df.index.time >= afternoon_begin]
        elif afternoon_begin is None:
            return df.loc[df.index.time < morning_end]
        else:
            if margin == 'left':
                return df.loc[(df.index.time < morning_end) | (df.index.time >= afternoon_begin)]
            else:
                raise NotImplementedError()

    @staticmethod
    def del_untrade_time(df: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]], cut_tail=True,
                         strip: str = None,
                         drop_last_row=False,
                         split_df=False):
        """

        Parameters
        ----------
        df
        cut_tail
            å»æ‰å°¾ç›˜3minç«ä»·
        strip
            å»æ‰å¼€æ”¶ç›˜n minçš„æ•°æ®
        split_df
            æ˜¯å¦è¿”å›ä¸Šä¸‹åˆåˆ†å¼€çš„df

        Returns
        -------

        """

        def _meta(df, cut_tail, strip, split_df):
            original_date = config.date
            current_yyyy, current_mm, current_dd = str(df.index[0].date()).split('-')
            update_date(current_yyyy, current_mm, current_dd)
            # date_today=df.index[0].date()
            # self.daily_timedeltas

            start_time = config.important_times['continues_auction_am_start']
            end_time = config.important_times['close_call_auction_end'] if not cut_tail else config.important_times[
                'continues_auction_pm_end']
            if strip is not None:
                strip_timedelta = str2timedelta(strip)
                start_time = config.important_times['continues_auction_am_start'] + strip_timedelta
                end_time = min(config.important_times['close_call_auction_end'] - strip_timedelta, end_time)
            a = df.loc[str(start_time):str(config.important_times['continues_auction_am_end'])]
            b = df.loc[str(config.important_times['continues_auction_pm_start']):str(end_time)]
            if split_df:
                a = a.sort_index()
                b = b.sort_index()
                if drop_last_row:
                    b = b.iloc[:-1]
                temp = (a, b)
            else:
                temp = pd.concat([a, b], axis=0)
                temp = temp.sort_index()
                if drop_last_row:
                    temp = temp.iloc[:-1]

            if original_date is not None:
                update_date(original_date[:4], original_date[4:6], original_date[6:])
            else:
                update_date(None, None, None)

            return temp

        if isinstance(df, list):
            return [_meta(_df, cut_tail=cut_tail, strip=strip, split_df=split_df) for _df in df]
        elif isinstance(df, pd.DataFrame) or isinstance(df, pd.Series):
            return _meta(df, cut_tail=cut_tail, strip=strip, split_df=split_df)

    @staticmethod
    def add_head_tail(df, head_timestamp=None, tail_timestamp=None, date_=None, bfill_head=False):
        """

        Parameters
        ----------
        df :
        head_timestamp :
        tail_timestamp :
        date_ :
        bfill_head : bool,
            æ˜¯å¦è¦å°†ç¬¬ä¸€è¡Œç”¨ç¬¬äºŒè¡Œçš„æ•°æ®fill

        Returns
        -------

        """
        if head_timestamp is None and tail_timestamp is None and date_ is None:
            raise ValueError(
                f"at least one of the params `date_` and (`head_timestamp`,`tail_timestamp`) should be not None")
        elif head_timestamp is None and tail_timestamp is None:
            # date_ is not none
            head_timestamp = pd.to_datetime(f"{date_} 09:30:00")
            tail_timestamp = pd.to_datetime(f"{date_} 14:57:00")
        else:
            raise ValueError("you should provide (`head_timestamp`,`tail_timestamp`) simultaneously")
        try:
            assert df.index[0] >= head_timestamp and df.index[-1] <= tail_timestamp
        except Exception as e:
            print('add_head_tail', df.index[0], head_timestamp, df.index[-1], tail_timestamp)
            raise e

        res = df.copy(deep=True)
        res.loc[pd.to_datetime(tail_timestamp)] = df.iloc[-1].copy(deep=True)
        if bfill_head:
            res.loc[pd.to_datetime(head_timestamp)] = df.iloc[0].copy(deep=True)
        else:
            res.loc[pd.to_datetime(head_timestamp)] = np.nan
        res = res.sort_index()
        return res

    @staticmethod
    def split_by_trade_period(df):
        res = [df.loc[s:e] for s, e in config.ranges]
        res = [LobTimePreprocessor.add_head_tail(cobh, head_timestamp=pd.to_datetime(s),
                                                 tail_timestamp=pd.to_datetime(e)) for cobh, (s, e) in
               zip(res, config.ranges)]
        return res

    @staticmethod
    def change_freq(df, freq):
        """deprecated"""
        logging.warning("change_freq deprecated", DeprecationWarning)
        return df.asfreq(freq='10ms', method='ffill').asfreq(freq=freq)

    @staticmethod
    def align_price(df, prices):
        pass

    # @staticmethod
    # def split_change_freq(df,freq='200ms'):
    #     dfs=LobTimePreprocessor.split_by_trade_period(df)
    #     clean_obhs


class LobFeatureEngineering(object):
    """è®¡ç®—è®¢å•ç°¿æˆªé¢å› å­.

    Attributes
    ----------


    """

    def __init__(self, curr='current', ColFormatter=LobColTemplate, limit_ratio=0.2):
        """

        Parameters
        ----------
        curr : str
            æœ€æ–°ä»·åˆ—å
        ColFormatter :
        limit_ratio :
            æ¶¨è·Œåœ
        """
        self.ap = {k: str(ColFormatter('a', k, 'p')) for k in range(1, 11)}
        self.bp = {k: str(ColFormatter('b', k, 'p')) for k in range(1, 11)}
        self.av = {k: str(ColFormatter('a', k, 'v')) for k in range(1, 11)}
        self.bv = {k: str(ColFormatter('b', k, 'v')) for k in range(1, 11)}
        self.curr = curr
        self.limit_ratio = limit_ratio

    def calc_reaches_limit(self, df):
        """
        todo åˆ©ç”¨æ˜¨æ”¶è®¡ç®—æ¶¨è·Œåœé˜ˆå€¼

        Parameters
        ----------
        df :

        Returns
        -------

        """
        # _open=df[self.curr].iloc[0]
        # upper=_open*
        pass

    def calc_buy_intense(self):
        """
        ä¹°å…¥æ„æ„¿å› å­ã€Šé«˜é¢‘å› å­çš„ç°å®ä¸å¹»æƒ³ã€‹ï¼Œéœ€è¦åˆ©ç”¨9:30å¼€ç›˜ååŠå°æ—¶å†…çš„æ•°æ®æ„å»ºè¯¥å› å­
        :return:
        """
        pass

    def calc_wap(self, df, level, cross=True):
        """Function to calculate level WAP

        References
        ----------
        [1] optiveré‡‘ç‰Œç®—æ³•. https://mp.weixin.qq.com/s/Pe4i3I9-ErYFE9uL5B5pvQ
        """
        if cross:
            wap = (df[self.bp[level]] * df[self.av[level]] + df[self.ap[level]] * df[self.bv[level]]) / (
                    df[self.bv[level]] + df[self.av[level]])
        else:
            wap = (df[self.ap[level]] * df[self.av[level]] + df[self.bp[level]] * df[self.bv[level]]) / (
                    df[self.bv[level]] + df[self.av[level]])
        name = f'wap{level}'
        if cross: name += '_cross'
        wap = wap.rename(name)
        wap = wap.replace([np.inf, -np.inf], np.nan)
        return wap

    def calc_cum_wap(self, df, level, cross=True):
        """Function to calculate corresponding weighted average price. å„æ¡£ä½volume*priceçš„åŠ æƒï¼Œè€Œéä¸€æ¡£ä»·æ ¼æŒ‰ç…§volumeåŠ æƒ

        Notes
        -----
        è¿™ä¸ªå‡½æ•°å¯èƒ½å¯¼è‡´ä»·æ ¼ç–¯ç‹‚è·³åŠ¨ï¼Œå› ä¸ºé«˜æ¡£ä½çš„æŠ¥æ’¤å•æ˜¯éå¸¸é¢‘ç¹çš„

        References
        ----------
        [1] optiveré‡‘ç‰Œç®—æ³•. https://mp.weixin.qq.com/s/Pe4i3I9-ErYFE9uL5B5pvQ
        """
        a_v = 0
        b_v = 0
        a_pv = 0
        b_pv = 0
        for i in range(1, level + 1):
            b_v += df[self.bv[i]]
            a_v += df[self.av[i]]
            if cross:
                a_pv += df[self.ap[level]] * df[self.bv[level]]
                b_pv += df[self.bp[level]] * df[self.av[level]]
            else:
                a_pv += df[self.ap[level]] * df[self.av[level]]
                b_pv += df[self.bp[level]] * df[self.bv[level]]
        wap = (a_pv + b_pv) / (a_v + b_v)
        name = f'cum_wap{level}'
        if cross: name += '_c'
        wap = wap.rename(name)
        wap = wap.replace([np.inf, -np.inf], np.nan)
        return wap

    def calc_cum_vol_wap(self, df, cum_level, cross=True):
        """ç”¨å¯¹æ‰‹sideç´¯ç§¯volumeæ¥åŠ æƒ1æ¡£ä»·æ ¼

        Parameters
        ----------
        df :
        cum_level : int,
            ç´¯ç§¯levelä¸ªæ¡£ä½çš„volume
        cross :

        Returns
        -------

        """
        a_v = 0
        b_v = 0
        for i in range(1, cum_level + 1):
            b_v += df[self.bv[i]]
            a_v += df[self.av[i]]

        if cross:
            wap = (df[self.bp[1]] * a_v + df[self.ap[1]] * b_v) / (a_v + b_v)
        else:
            wap = (df[self.bp[1]] * b_v + df[self.ap[1]] * a_v) / (a_v + b_v)
        name = f'cum_vol_wap{cum_level}'
        if cross: name += '_c'
        wap = wap.rename(name)
        wap = wap.replace([np.inf, -np.inf], np.nan)
        return wap

    def calc_mid_price(self, df, process_zeros=True):
        """

        Parameters
        ----------
        df :
        process_zeros :
            ç±³ç­æ•°æ®å¯èƒ½ä¼šåœ¨æ¶¨åœæ—¶æ˜¾ç¤ºbuy priceä¸º0.å¤„ç†é€»è¾‘ä¸º

        Returns
        -------

        """
        idx_bp_zero = df[self.bp[1]] == 0
        idx_ap_zero = df[self.ap[1]] == 0
        if process_zeros:
            # if np.logical_and(idx_bp_zero,idx_ap_zero).any(): raise ValueError()
            # fill ask price
            idx = np.logical_and(idx_ap_zero, ~idx_bp_zero)
            df.loc[idx, self.ap[1]] = df[self.bp[1]].loc[idx] + 0.01  # add a tick
            # fill bid price
            idx = np.logical_and(idx_bp_zero, ~idx_ap_zero)
            df.loc[idx, self.bp[1]] = df[self.ap[1]].loc[idx] - 0.01
        return (df[self.ap[1]] + df[self.bp[1]]).rename('mid_price') / 2

    def calc_spread(self, df):
        return (df[self.ap[1]] - df[self.bp[1]]).rename('spread')

    def calc_relative_spread(self, df):
        return (self.calc_spread(df) / self.calc_mid_price(df)).rename('relative_spread')

    def calc_price_breadth(self, df):
        """

        :param df:
        :return:

        .. [#] High-Frequency TradingAspects of market liquidityï¼ˆBervasï¼Œ2006ï¼‰
        """

        pb_b = (df[self.curr] - df[self.bp[1]]).rename('price_breadth_b')
        pb_a = (df[self.ap[1]] - df[self.curr]).rename('price_breadth_a')
        return pb_b, pb_a

    def calc_length_imbalance(self, df, level):
        """
        quantity imbalance

        :return:

        .. [#] Cao, C., Hansch, O., & Wang, X. (2009). The information content of an open limitâ€order book. Journal of Futures Markets: Futures, Options, and Other Derivative Products, 29(1), 16-41. ç¬¬30é¡µ
        """
        QR_level = (df[self.av[level]] - df[self.bv[level]]) / (df[self.av[level]] + df[self.bv[level]])
        QR_level = pd.Series(QR_level, index=df.index, name=f"li_{level}")
        return QR_level

    def calc_height_imbalance(self, df, level):
        """
        å’ŒåŸæ–‡æœ‰å‡ºå…¥ï¼ŒæŒ‰ç…§åŸæ–‡å…¬å¼å¯èƒ½ä¼šå‡ºç°infï¼Œå› ä¸ºåˆ†æ¯ä¸º0.æ ¹æ®æ–‡ç« æ‰€è¡¨è¾¾å«ä¹‰æˆ‘è¿›è¡Œäº†bid priceè®¡ç®—éƒ¨åˆ†çš„ä¿®æ”¹

        :return:

        .. [#] Cao, C., Hansch, O., & Wang, X. (2009). The information content of an open limitâ€order book. Journal of Futures Markets: Futures, Options, and Other Derivative Products, 29(1), 16-41. ç¬¬30é¡µ
        """
        assert level >= 2
        # åŸæ–‡
        # nominator = (df[self.ap[level]] - df[self.ap[level - 1]]) - (df[self.bp[level]] - df[self.bp[level - 1]])
        # denominator = (df[self.ap[level]] - df[self.ap[level - 1]]) + (df[self.bp[level]] - df[self.bp[level - 1]])
        # ä¿®æ”¹
        nominator = (df[self.ap[level]] - df[self.ap[level - 1]]) - (df[self.bp[level - 1]] - df[self.bp[level]])
        denominator = (df[self.ap[level]] - df[self.ap[level - 1]]) + (df[self.bp[level - 1]] - df[self.bp[level]])
        HR_level = pd.Series(nominator / denominator, index=df.index, name=f"hi_{level}")
        return HR_level

    def calc_spread_tick(self, df, num_levels=5):
        """
        ç›˜å£ä¿¡æ¯æ˜¯åˆ¤æ–­ä¸ªè‚¡çŸ­æœŸèµ°åŠ¿çš„é‡è¦ä¾æ®ï¼Œ
        çŸ­æœŸçš„å¸‚åœºä»·æ ¼æ˜¯å–å†³äºå½“å‰ ä¹°ç›˜ éœ€æ±‚é‡
        å’Œå–ç›˜ ä¾›ç»™ é‡æ„å»ºçš„å‡è¡¡ä»·æ ¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ¤æ–­ ç›˜å£ ä¹°å–æŒ‚å•çš„ç›¸å¯¹å¼ºå¼±å¯¹äºè‚¡ç¥¨ä»·æ ¼çš„
        çŸ­æœŸèµ°åŠ¿å…·æœ‰ç»Ÿè®¡æ„ä¹‰ä¸Šæ˜¾è‘—çš„é¢„åˆ¤ä½œç”¨ã€‚ å½“éœ€æ±‚è¿œå¤§äºä¾›ç»™æ—¶ï¼Œå‡è¡¡ä»·æ ¼å°†ä¸Šç§»ï¼› ç›¸å
        åœ°ï¼Œ å½“ä¾›ç»™è¿œå¤§äºéœ€æ±‚æ—¶ï¼Œå‡è¡¡ä»·æ ¼å°†ä¸‹é™ã€‚

        .. math::
            bid=\\Sigma_{i=1}^{num\\_levels}bidprice_i*bidvol_i*w_i\\newline
            ask=\\Sigma_{i=1}^{num\\_levels}askprice_i*askvol_i*w_i\\newline
            w_i=1-\\frac{i-1}{num\\_levels}\\newline
            spread\\_tick=(bid-ask)/(bid+ask)

        .. [#] 2019-09-05_å¤©é£è¯åˆ¸_å¸‚åœºå¾®è§‚ç»“æ„æ¢æç³»åˆ—ä¹‹äºŒï¼šè®¢å•ç°¿ä¸Šçš„alpha

        :param num_levels: æ¡£ä½æ•°é‡
        :return:
        """
        weights = {}
        for i in range(1, num_levels + 1):
            weights[i] = 1 - (i - 1) / num_levels

        bid = ask = 0
        for i in range(1, num_levels + 1):
            bid += df[self.bp[i]] * df[self.bv[i]] * weights[i]
            ask += df[self.ap[i]] * df[self.av[i]] * weights[i]

        spread_tick = (bid - ask) / (bid + ask)
        spread_tick = spread_tick.rename('spread_tick')
        return spread_tick

    def calc_realized_volatility(self, series):
        cum_vol = np.cumsum((series + 1) ** 2)
        denominator = np.arange(1, len(series) + 1, 1)
        realized_vol = (cum_vol / denominator - 1).rename('realized_vol')
        return realized_vol

    def calc_volume_order_imbalance(self, df: pd.DataFrame):
        """
        .. [#] Reference From <Order imbalance Based Strategy in High Frequency Trading>
        """
        current_bid_price = df[self.bp[1]]
        bid_price_diff = current_bid_price - current_bid_price.shift()
        current_bid_vol = df[self.bv[1]]
        bvol_diff = current_bid_vol - current_bid_vol.shift()
        # å³ç»“åˆï¼Œæœ€åä¸€ä¸ªwhereæ˜¯ç›´æ¥returnæ–°çš„arrayï¼Œè€Œä¸æ˜¯å»ä¿®æ”¹bid_price_diff
        bid_increment = np.where(bid_price_diff > 0, current_bid_vol,
                                 np.where(bid_price_diff < 0, 0,
                                          np.where(bid_price_diff == 0, bvol_diff, bid_price_diff)))

        current_ask_price = df[self.ap[1]]
        ask_price_diff = current_ask_price - current_ask_price.shift()
        current_ask_vol = df[self.av[1]]
        avol_diff = current_ask_vol - current_ask_vol.shift()
        ask_increment = np.where(ask_price_diff < 0, current_ask_vol,
                                 np.where(ask_price_diff > 0, 0,
                                          np.where(ask_price_diff == 0, avol_diff, ask_price_diff)))

        res = pd.Series(bid_increment - ask_increment, index=df.index, name='voi')

        return res

    def calc_buy_sell_pressure(self, df, level1, level2, method='MID'):
        """
        åˆ©ç”¨mid priceè®¡ç®—ä¹°å–å‹åŠ›å·®
        :param df:
        :param level1:
        :param level2:
        :param method:
        :return:

        References
        ----------
            .. [#] [å¤©é£è¯åˆ¸_ä¹°å–å‹åŠ›å¤±è¡¡â€”â€”åˆ©ç”¨é«˜é¢‘æ•°æ®æ‹“å±•ç›˜å£æ•°æ®](https://bigquant.com/wiki/pdfjs/web/viewer.html?file=/wiki/static/upload/2b/2bc961b3-e365-4afb-aa98-e9f9d9fa299e.pdf)
        """
        assert level1 < level2
        levels = np.arange(level1, level2 + 1)
        if method == "MID":
            M = self.calc_mid_price(df)
        else:
            raise NotImplementedError('price_weighted_pressure')

        # éœ€è¦æ³¨æ„ï¼Œå¦‚æœä¸é€‚ç”¨middle priceé‚£ä¹ˆå¯èƒ½åˆ†æ¯ä¼šå‡ºç°nan
        bid_d = np.array([M / (M - df[self.bp[s]]) for s in levels])
        # bid_d = [_.replace(np.inf,0) for _ in bid_d]
        idx = np.isinf(bid_d)
        bid_d[idx] = np.nan
        bid_denominator = np.nansum(bid_d, axis=0).reshape(-1, 1).T
        bid_weights = np.divide(bid_d, bid_denominator, where=~np.isnan(bid_denominator))
        press_buy = sum([df[self.bv[i + 1]] * w for i, w in enumerate(bid_weights)])

        ask_d = np.array([M / (df[self.ap[s]] - M) for s in levels])
        # ask_d = [_.replace(np.inf,0) for _ in ask_d]
        idx = np.isinf(ask_d)
        ask_d[idx] = np.nan
        ask_denominator = np.nansum(ask_d, axis=0).reshape(-1, 1).T
        ask_weights = np.divide(ask_d, ask_denominator, where=~np.isnan(ask_denominator))
        press_sell = sum([df[self.av[i + 1]] * w for i, w in enumerate(ask_weights)])

        res = pd.Series((press_buy - press_sell).replace([-np.inf, np.inf], np.nan), name='buy_sell_pressure')
        return res

    def calc_gaps(self, df, level=5):
        """

        :param df:
        :return:

        .. [#] Price jump prediction in Limit Order Book. Ban Zheng, Eric Moulines, Fr Ìed Ìeric Abergel https://arxiv.org/pdf/1204.1381.pdf
        """

        def _meta_price_gap(df, i, side: str):
            """price gap"""
            res = None
            if side == 'b':
                res = df[self.bp[i + 1]] - df[self.bp[i]]
            elif side == 'a':
                res = df[self.ap[i + 1]] - df[self.ap[i]]
            return res.rename(f"{side}{i + 1}{i}_p_gap")

        # res = pd.DataFrame()
        l = []
        for i in range(level - 1, 0, -1):
            gap = _meta_price_gap(df, i, 'a')
            l.append(gap)
        for i in range(1, level):
            gap = _meta_price_gap(df, i, 'b')
            l.append(gap)
        res = pd.concat(l, axis=1)
        return res

    # def calc_event_dummies(self, trade_details,order_details):
    #     """
    #     .. [#] Price jump prediction in Limit Order Book. Ban Zheng, Eric Moulines, Fr Ìed Ìeric Abergel https://arxiv.org/pdf/1204.1381.pdf
    #
    #     :param events: a Series of integers indicating the type of events
    #     :param df:
    #     :return: one-hot dummies
    #
    #     """
    #     vs = ["BLO", "ALO", "BMO", "AMO", "BTT", "ATT"]
    #     mapper = {k+1: vs[k] for k in range(6)}
    #     event_df = pd.get_dummies(events.apply(lambda x: mapper[x]))
    #     return event_df

    def calc_bid_ask_volume_ratio(self, df: pd.DataFrame, level=5):
        """

        :param df:
        :param level:
        :return:

        .. [x] Price jump prediction in Limit Order Book. Ban Zheng, Eric Moulines,FrÂ´edÂ´eric Abergel
        """
        df1 = deepcopy(df)
        cols = [self.av[i] for i in range(1, level + 1)] + [self.bv[i] for i in range(1, level + 1)]
        # df1.loc[:,cols] = np.exp(df1[cols])

        bavr = 1
        res = pd.DataFrame()
        for i in range(1, level + 1):
            bavr *= np.clip(df1[self.bv[i]] / df1[self.av[i]], a_min=-32,
                            a_max=32)  # limit the value between the range of float32
            W_i = pd.Series(bavr, name=f'bid_ask_volume_ratio{i}')
            res = pd.concat([res, W_i], axis=1)

        return res

    def calc_window_avg_order_amount(self):
        """
         ä¸€æ®µæ—¶é—´çª—å£å†…çš„å¹³å‡è®¢å•ç¬”æ•°
        :return:
        """
        pass

    def calc_window_avg_trade_amount(self):
        """
         ä¸€æ®µæ—¶é—´çª—å£å†…çš„å¹³å‡æˆäº¤ç¬”æ•°
        :return:
        """
        pass

    def calc_window_avg_order_vol(self):
        """
         ä¸€æ®µæ—¶é—´çª—å£å†…çš„å¹³å‡è®¢å•é‡
        :return:
        """
        pass

    def calc_window_avg_trade_vol(self):
        """
         ä¸€æ®µæ—¶é—´çª—å£å†…çš„å¹³å‡æˆäº¤é‡
        :return:
        """
        pass

    def calc_window_weighted_avg_order_vol(self):
        """
         ä¸€æ®µæ—¶é—´çª—å£å†…çš„åŠ æƒå¹³å‡è®¢å•é‡
        :return:
        """
        pass

    def calc_window_weighted_avg_trade_vol(self):
        """
         ä¸€æ®µæ—¶é—´çª—å£å†…çš„åŠ æƒå¹³å‡æˆäº¤é‡
        :return:
        """
        pass

    def calc_momentum(self):
        """
         è®¡ç®—ä¸€äº›åŠ¨é‡å› å­
        :return:
        """
        pass

    def generate_cross_section(self, clean_obh, level=5):
        """
        è®¡ç®—æˆªé¢featuresï¼Œä¸åŒ…å«ä¸åŒæ—¶åˆ»æ•°æ®æ‰€æ„å»ºçš„å› å­
        :param clean_obh:
        :param level:
        :return:
        """

        self.mp = self.calc_mid_price(clean_obh)
        # å¥½åƒæ²¡å•¥ç”¨ï¼Œå› ä¸ºåœ¨10msçš„æ•°æ®ä¸­ï¼Œè¯¥åˆ—ç»å¤§éƒ¨åˆ†éƒ½æ˜¯0ã€‚ä½†å¥½åƒå¦‚æœè¿™ä¹ˆçœ‹ï¼Œé‚£ä¹ˆæ‰€æœ‰å› å­éƒ½æ˜¯ç¨€ç–çš„ï¼Œå› ä¸ºdiffåå¤§éƒ¨åˆ†æ—¶é—´éƒ½æ˜¯0
        self.spread = self.calc_spread(clean_obh)
        self.breadth_b, self.breadth_a = self.calc_price_breadth(clean_obh)
        self.rs = self.calc_relative_spread(clean_obh)
        self.st = self.calc_spread_tick(clean_obh, num_levels=level)
        self.voi = self.calc_volume_order_imbalance(clean_obh)
        self.bsp = self.calc_buy_sell_pressure(clean_obh, level1=1, level2=level, method='MID')
        self.gaps = self.calc_gaps(clean_obh, level=level)
        self.bavr = self.calc_bid_ask_volume_ratio(clean_obh, level=level)

        self.waps = [self.calc_wap(clean_obh, level=i, cross=True) for i in range(1, level + 1)] + [
            self.calc_wap(clean_obh, level=i, cross=False) for i in range(1, level + 1)]
        self.lis = [self.calc_length_imbalance(clean_obh, level=i) for i in range(1, level)]
        self.his = [self.calc_height_imbalance(clean_obh, level=i) for i in range(2, level)]

        self.mp_ret = np.log(self.mp, where=np.logical_and(~np.isnan(self.mp), self.mp > 0)).diff().rename("mid_p_ret")
        self.wap1_ret = np.log(self.waps[0],
                               where=np.logical_and(~np.isnan(self.waps[0]), self.waps[0] > 0)).diff().rename(
            "wap1_ret")
        self.wap2_ret = np.log(self.waps[1],
                               where=np.logical_and(~np.isnan(self.waps[1]), self.waps[1] > 0)).diff().rename(
            "wap2_ret")

        self.features = pd.concat(
            self.waps
            + self.lis
            + self.his
            + [self.mp,
               self.spread,
               self.breadth_b,
               self.breadth_a,
               self.rs,
               self.st,
               self.voi,
               self.bsp,
               # self.volatility, # ä¼šé€æ¸å‡å°ä¸º0
               self.gaps,
               self.bavr,
               self.mp_ret,
               self.wap1_ret,
               self.wap2_ret,
               ],
            axis=1)
        # fixme å°†2 level headerè½¬ä¸º1 level
        # df_feature.columns = ['_'.join(col) for col in df_feature.columns]  # time_id is changed to time_id_

        return self.features

    # def agg_features(self, features: pd.DataFrame, agg_freq: str):
    #     features = features.resample(agg_freq).agg([np.mean, np.std, np.median])
    #     return features


class ImbalancedDataPreprocessor(BaseDataPreprocessor):
    """
    References
    ----------
    [1] Werner de Vargas, V., Schneider Aranda, J.A., dos Santos Costa, R. et al. Imbalanced data preprocessing techniques for machine learning: a systematic mapping study. Knowl Inf Syst 65, 31â€“57 (2023). https://doi.org/10.1007/s10115-022-01772-8
    """

    def undersampling(self):
        pass

    def oversampling(self):
        pass

    def hybrid_sampling(self):
        pass


if __name__ == '__main__':
    pass
